<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>10 Week 5 Lab | Biometry Lecture and Lab Notes</title>
  <meta name="description" content="10 Week 5 Lab | Biometry Lecture and Lab Notes" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="10 Week 5 Lab | Biometry Lecture and Lab Notes" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="10 Week 5 Lab | Biometry Lecture and Lab Notes" />
  
  
  

<meta name="author" content="Heather Lynch" />


<meta name="date" content="2021-03-04" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="week-5-lecture.html"/>
<link rel="next" href="week-6-lecture.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Biometry Lecture and Lab Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="week-1-lecture.html"><a href="week-1-lecture.html"><i class="fa fa-check"></i><b>1</b> Week 1 Lecture</a><ul>
<li class="chapter" data-level="1.1" data-path="week-1-lecture.html"><a href="week-1-lecture.html#reading-material"><i class="fa fa-check"></i><b>1.1</b> Reading Material</a></li>
<li class="chapter" data-level="1.2" data-path="week-1-lecture.html"><a href="week-1-lecture.html#basic-outline"><i class="fa fa-check"></i><b>1.2</b> Basic Outline</a></li>
<li class="chapter" data-level="1.3" data-path="week-1-lecture.html"><a href="week-1-lecture.html#todays-agenda"><i class="fa fa-check"></i><b>1.3</b> Today's Agenda</a></li>
<li class="chapter" data-level="1.4" data-path="week-1-lecture.html"><a href="week-1-lecture.html#basic-probability-theory"><i class="fa fa-check"></i><b>1.4</b> Basic Probability Theory</a><ul>
<li class="chapter" data-level="1.4.1" data-path="week-1-lecture.html"><a href="week-1-lecture.html#intersection"><i class="fa fa-check"></i><b>1.4.1</b> Intersection</a></li>
<li class="chapter" data-level="1.4.2" data-path="week-1-lecture.html"><a href="week-1-lecture.html#union"><i class="fa fa-check"></i><b>1.4.2</b> Union</a></li>
<li class="chapter" data-level="1.4.3" data-path="week-1-lecture.html"><a href="week-1-lecture.html#complement"><i class="fa fa-check"></i><b>1.4.3</b> Complement:</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="week-1-lecture.html"><a href="week-1-lecture.html#multiple-events"><i class="fa fa-check"></i><b>1.5</b> Multiple events</a></li>
<li class="chapter" data-level="1.6" data-path="week-1-lecture.html"><a href="week-1-lecture.html#conditionals"><i class="fa fa-check"></i><b>1.6</b> Conditionals</a></li>
<li class="chapter" data-level="1.7" data-path="week-1-lecture.html"><a href="week-1-lecture.html#bayes-theorem"><i class="fa fa-check"></i><b>1.7</b> Bayes Theorem</a></li>
<li class="chapter" data-level="1.8" data-path="week-1-lecture.html"><a href="week-1-lecture.html#a-few-foundational-ideas"><i class="fa fa-check"></i><b>1.8</b> A few foundational ideas</a></li>
<li class="chapter" data-level="1.9" data-path="week-1-lecture.html"><a href="week-1-lecture.html#overview-of-univariate-distributions"><i class="fa fa-check"></i><b>1.9</b> Overview of Univariate Distributions</a></li>
<li class="chapter" data-level="1.10" data-path="week-1-lecture.html"><a href="week-1-lecture.html#what-can-you-ask-of-a-distribution"><i class="fa fa-check"></i><b>1.10</b> What can you ask of a distribution?</a><ul>
<li class="chapter" data-level="1.10.1" data-path="week-1-lecture.html"><a href="week-1-lecture.html#expected-value-of-a-random-variable"><i class="fa fa-check"></i><b>1.10.1</b> Expected Value of a Random Variable</a></li>
<li class="chapter" data-level="1.10.2" data-path="week-1-lecture.html"><a href="week-1-lecture.html#discrete-case"><i class="fa fa-check"></i><b>1.10.2</b> Discrete Case</a></li>
<li class="chapter" data-level="1.10.3" data-path="week-1-lecture.html"><a href="week-1-lecture.html#continuous-case"><i class="fa fa-check"></i><b>1.10.3</b> Continuous Case</a></li>
</ul></li>
<li class="chapter" data-level="1.11" data-path="week-1-lecture.html"><a href="week-1-lecture.html#a-brief-introduction-to-scientific-method"><i class="fa fa-check"></i><b>1.11</b> A Brief Introduction to Scientific Method</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="week-1-lab-handout.html"><a href="week-1-lab-handout.html"><i class="fa fa-check"></i><b>2</b> Week 1 Lab Handout</a><ul>
<li class="chapter" data-level="2.1" data-path="week-1-lab-handout.html"><a href="week-1-lab-handout.html#using-r-like-a-calculator"><i class="fa fa-check"></i><b>2.1</b> Using R like a calculator</a></li>
<li class="chapter" data-level="2.2" data-path="week-1-lab-handout.html"><a href="week-1-lab-handout.html#the-basic-data-structures-in-r"><i class="fa fa-check"></i><b>2.2</b> The basic data structures in R</a></li>
<li class="chapter" data-level="2.3" data-path="week-1-lab-handout.html"><a href="week-1-lab-handout.html#writing-functions-in-r"><i class="fa fa-check"></i><b>2.3</b> Writing functions in R</a></li>
<li class="chapter" data-level="2.4" data-path="week-1-lab-handout.html"><a href="week-1-lab-handout.html#writing-loops-and-ifelse"><i class="fa fa-check"></i><b>2.4</b> Writing loops and if/else</a></li>
<li class="chapter" data-level="2.5" data-path="week-1-lab-handout.html"><a href="week-1-lab-handout.html#a-short-diversion-bias-in-estimators"><i class="fa fa-check"></i><b>2.5</b> (A short diversion) Bias in estimators</a></li>
<li class="chapter" data-level="2.6" data-path="week-1-lab-handout.html"><a href="week-1-lab-handout.html#lesson-6-some-practice-writing-r-code"><i class="fa fa-check"></i><b>2.6</b> Lesson #6: Some practice writing R code</a></li>
<li class="chapter" data-level="2.7" data-path="week-1-lab-handout.html"><a href="week-1-lab-handout.html#a-few-final-notes"><i class="fa fa-check"></i><b>2.7</b> A few final notes</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="week-2-lecture.html"><a href="week-2-lecture.html"><i class="fa fa-check"></i><b>3</b> Week 2 Lecture</a><ul>
<li class="chapter" data-level="3.1" data-path="week-2-lecture.html"><a href="week-2-lecture.html#hypothesis-testing-and-p-values"><i class="fa fa-check"></i><b>3.1</b> Hypothesis testing and p-values</a></li>
<li class="chapter" data-level="3.2" data-path="week-2-lecture.html"><a href="week-2-lecture.html#permutation-tests"><i class="fa fa-check"></i><b>3.2</b> Permutation tests</a></li>
<li class="chapter" data-level="3.3" data-path="week-2-lecture.html"><a href="week-2-lecture.html#parameter-estimation"><i class="fa fa-check"></i><b>3.3</b> Parameter estimation</a></li>
<li class="chapter" data-level="3.4" data-path="week-2-lecture.html"><a href="week-2-lecture.html#method-1-non-parametric-bootstrap"><i class="fa fa-check"></i><b>3.4</b> Method #1: Non-parametric bootstrap</a></li>
<li class="chapter" data-level="3.5" data-path="week-2-lecture.html"><a href="week-2-lecture.html#parametric-bootstrap"><i class="fa fa-check"></i><b>3.5</b> Parametric bootstrap</a></li>
<li class="chapter" data-level="3.6" data-path="week-2-lecture.html"><a href="week-2-lecture.html#jackknife"><i class="fa fa-check"></i><b>3.6</b> Jackknife</a></li>
<li class="chapter" data-level="3.7" data-path="week-2-lecture.html"><a href="week-2-lecture.html#jackknife-after-bootstrap"><i class="fa fa-check"></i><b>3.7</b> Jackknife-after-bootstrap</a></li>
<li class="chapter" data-level="3.8" data-path="week-2-lecture.html"><a href="week-2-lecture.html#by-the-end-of-week-2-you-should-understand..."><i class="fa fa-check"></i><b>3.8</b> By the end of Week 2, you should understand...</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="week-2-lab.html"><a href="week-2-lab.html"><i class="fa fa-check"></i><b>4</b> Week 2 Lab</a><ul>
<li class="chapter" data-level="4.1" data-path="week-2-lab.html"><a href="week-2-lab.html#confidence-intervals"><i class="fa fa-check"></i><b>4.1</b> Confidence intervals</a></li>
<li class="chapter" data-level="4.2" data-path="week-2-lab.html"><a href="week-2-lab.html#testing-hypotheses-through-permutation"><i class="fa fa-check"></i><b>4.2</b> Testing hypotheses through permutation</a></li>
<li class="chapter" data-level="4.3" data-path="week-2-lab.html"><a href="week-2-lab.html#basics-of-bootstrap-and-jackknife"><i class="fa fa-check"></i><b>4.3</b> Basics of bootstrap and jackknife</a></li>
<li class="chapter" data-level="4.4" data-path="week-2-lab.html"><a href="week-2-lab.html#calculating-bias-and-standard-error"><i class="fa fa-check"></i><b>4.4</b> Calculating bias and standard error</a></li>
<li class="chapter" data-level="4.5" data-path="week-2-lecture.html"><a href="week-2-lecture.html#parametric-bootstrap"><i class="fa fa-check"></i><b>4.5</b> Parametric bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="week-3-lecture.html"><a href="week-3-lecture.html"><i class="fa fa-check"></i><b>5</b> Week 3 Lecture</a><ul>
<li class="chapter" data-level="5.1" data-path="week-3-lecture.html"><a href="week-3-lecture.html#overview-of-probability-distributions"><i class="fa fa-check"></i><b>5.1</b> Overview of probability distributions</a></li>
<li class="chapter" data-level="5.2" data-path="week-3-lecture.html"><a href="week-3-lecture.html#normal-gaussian-distribution"><i class="fa fa-check"></i><b>5.2</b> Normal (Gaussian) Distribution</a></li>
<li class="chapter" data-level="5.3" data-path="week-3-lecture.html"><a href="week-3-lecture.html#standard-normal-distribution"><i class="fa fa-check"></i><b>5.3</b> Standard Normal Distribution</a></li>
<li class="chapter" data-level="5.4" data-path="week-3-lecture.html"><a href="week-3-lecture.html#log-normal-distribution"><i class="fa fa-check"></i><b>5.4</b> Log-Normal Distribution</a></li>
<li class="chapter" data-level="5.5" data-path="week-3-lecture.html"><a href="week-3-lecture.html#intermission-central-limit-theorem"><i class="fa fa-check"></i><b>5.5</b> Intermission: Central Limit Theorem</a></li>
<li class="chapter" data-level="5.6" data-path="week-3-lecture.html"><a href="week-3-lecture.html#poisson-distribution"><i class="fa fa-check"></i><b>5.6</b> Poisson Distribution</a></li>
<li class="chapter" data-level="5.7" data-path="week-3-lecture.html"><a href="week-3-lecture.html#binomial-distribution"><i class="fa fa-check"></i><b>5.7</b> Binomial Distribution</a></li>
<li class="chapter" data-level="5.8" data-path="week-3-lecture.html"><a href="week-3-lecture.html#beta-distribution"><i class="fa fa-check"></i><b>5.8</b> Beta Distribution</a></li>
<li class="chapter" data-level="5.9" data-path="week-3-lecture.html"><a href="week-3-lecture.html#gamma-distribution"><i class="fa fa-check"></i><b>5.9</b> Gamma Distribution</a></li>
<li class="chapter" data-level="5.10" data-path="week-3-lecture.html"><a href="week-3-lecture.html#some-additional-notes"><i class="fa fa-check"></i><b>5.10</b> Some additional notes:</a></li>
<li class="chapter" data-level="5.11" data-path="week-3-lecture.html"><a href="week-3-lecture.html#by-the-end-of-week-3-you-should-understand..."><i class="fa fa-check"></i><b>5.11</b> By the end of Week 3, you should understand...</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="week-3-lab.html"><a href="week-3-lab.html"><i class="fa fa-check"></i><b>6</b> Week 3 Lab</a><ul>
<li class="chapter" data-level="6.1" data-path="week-3-lab.html"><a href="week-3-lab.html#exploring-the-univariate-distributions-with-r"><i class="fa fa-check"></i><b>6.1</b> Exploring the univariate distributions with R</a></li>
<li class="chapter" data-level="6.2" data-path="week-3-lab.html"><a href="week-3-lab.html#standard-deviation-vs.-standard-error"><i class="fa fa-check"></i><b>6.2</b> Standard deviation vs. Standard error</a></li>
<li class="chapter" data-level="6.3" data-path="week-3-lab.html"><a href="week-3-lab.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>6.3</b> The Central Limit Theorem</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="week-4-lecture.html"><a href="week-4-lecture.html"><i class="fa fa-check"></i><b>7</b> Week 4 Lecture</a><ul>
<li class="chapter" data-level="7.1" data-path="week-4-lecture.html"><a href="week-4-lecture.html#short-digression-degrees-of-freedom"><i class="fa fa-check"></i><b>7.1</b> Short digression: Degrees of freedom</a></li>
<li class="chapter" data-level="7.2" data-path="week-4-lecture.html"><a href="week-4-lecture.html#t-distribution"><i class="fa fa-check"></i><b>7.2</b> t-distribution</a></li>
<li class="chapter" data-level="7.3" data-path="week-4-lecture.html"><a href="week-4-lecture.html#chi-squared-distribution"><i class="fa fa-check"></i><b>7.3</b> Chi-squared distribution</a></li>
<li class="chapter" data-level="7.4" data-path="week-4-lecture.html"><a href="week-4-lecture.html#f-distribution"><i class="fa fa-check"></i><b>7.4</b> F distribution</a></li>
<li class="chapter" data-level="7.5" data-path="week-4-lecture.html"><a href="week-4-lecture.html#estimating-confidence-intervals---5-special-cases"><i class="fa fa-check"></i><b>7.5</b> Estimating confidence intervals - 5 special cases</a></li>
<li class="chapter" data-level="7.6" data-path="week-4-lecture.html"><a href="week-4-lecture.html#to-recap"><i class="fa fa-check"></i><b>7.6</b> To recap</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="week-4-lab.html"><a href="week-4-lab.html"><i class="fa fa-check"></i><b>8</b> Week 4 Lab</a></li>
<li class="chapter" data-level="9" data-path="week-5-lecture.html"><a href="week-5-lecture.html"><i class="fa fa-check"></i><b>9</b> Week 5 Lecture</a><ul>
<li class="chapter" data-level="9.1" data-path="week-5-lecture.html"><a href="week-5-lecture.html#statistical-power"><i class="fa fa-check"></i><b>9.1</b> Statistical power</a></li>
<li class="chapter" data-level="9.2" data-path="week-5-lecture.html"><a href="week-5-lecture.html#the-single-sample-t-test"><i class="fa fa-check"></i><b>9.2</b> The single sample t test</a></li>
<li class="chapter" data-level="9.3" data-path="week-5-lecture.html"><a href="week-5-lecture.html#the-unpaired-two-sample-t-test"><i class="fa fa-check"></i><b>9.3</b> The unpaired two sample t test</a></li>
<li class="chapter" data-level="9.4" data-path="week-5-lecture.html"><a href="week-5-lecture.html#the-paired-two-sample-t-test"><i class="fa fa-check"></i><b>9.4</b> The paired two sample t test</a></li>
<li class="chapter" data-level="9.5" data-path="week-5-lecture.html"><a href="week-5-lecture.html#the-f-test"><i class="fa fa-check"></i><b>9.5</b> The F test</a></li>
<li class="chapter" data-level="9.6" data-path="week-5-lecture.html"><a href="week-5-lecture.html#comparing-two-proportions"><i class="fa fa-check"></i><b>9.6</b> Comparing two proportions</a></li>
<li class="chapter" data-level="9.7" data-path="week-5-lecture.html"><a href="week-5-lecture.html#comparing-two-distributions"><i class="fa fa-check"></i><b>9.7</b> Comparing two distributions</a></li>
<li class="chapter" data-level="9.8" data-path="week-5-lecture.html"><a href="week-5-lecture.html#a-bit-more-detail-on-the-binomial"><i class="fa fa-check"></i><b>9.8</b> A bit more detail on the Binomial</a></li>
<li class="chapter" data-level="9.9" data-path="week-5-lecture.html"><a href="week-5-lecture.html#side-note-about-the-wald-test"><i class="fa fa-check"></i><b>9.9</b> Side-note about the Wald test</a></li>
<li class="chapter" data-level="9.10" data-path="week-5-lecture.html"><a href="week-5-lecture.html#chi-squared-goodness-of-fit-test"><i class="fa fa-check"></i><b>9.10</b> Chi-squared goodness-of-fit test</a></li>
<li class="chapter" data-level="9.11" data-path="week-5-lecture.html"><a href="week-5-lecture.html#chi-squared-test-of-independence"><i class="fa fa-check"></i><b>9.11</b> Chi-squared test of independence</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="week-5-lab.html"><a href="week-5-lab.html"><i class="fa fa-check"></i><b>10</b> Week 5 Lab</a><ul>
<li class="chapter" data-level="10.1" data-path="week-5-lab.html"><a href="week-5-lab.html#f-test"><i class="fa fa-check"></i><b>10.1</b> F-test</a></li>
<li class="chapter" data-level="10.2" data-path="week-5-lecture.html"><a href="week-5-lecture.html#comparing-two-proportions"><i class="fa fa-check"></i><b>10.2</b> Comparing two proportions</a></li>
<li class="chapter" data-level="10.3" data-path="week-5-lecture.html"><a href="week-5-lecture.html#comparing-two-distributions"><i class="fa fa-check"></i><b>10.3</b> Comparing two distributions</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="week-6-lecture.html"><a href="week-6-lecture.html"><i class="fa fa-check"></i><b>11</b> Week 6 Lecture</a></li>
<li class="chapter" data-level="12" data-path="week-6-lab.html"><a href="week-6-lab.html"><i class="fa fa-check"></i><b>12</b> Week 6 Lab</a></li>
<li class="chapter" data-level="13" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html"><i class="fa fa-check"></i><b>13</b> Week 7 Lecture/Lab</a><ul>
<li class="chapter" data-level="13.1" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#part-i-box-plots"><i class="fa fa-check"></i><b>13.1</b> PART I: Box plots</a></li>
<li class="chapter" data-level="13.2" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#part-ii-two-dimensional-data"><i class="fa fa-check"></i><b>13.2</b> PART II: Two-dimensional data</a></li>
<li class="chapter" data-level="13.3" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#part-iii-three-dimensional-data"><i class="fa fa-check"></i><b>13.3</b> PART III: Three-dimensional data</a></li>
<li class="chapter" data-level="13.4" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#part-iv-multiple-plots"><i class="fa fa-check"></i><b>13.4</b> PART IV: Multiple plots</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="week-8-lecture.html"><a href="week-8-lecture.html"><i class="fa fa-check"></i><b>14</b> Week 8 Lecture</a><ul>
<li class="chapter" data-level="14.1" data-path="week-8-lecture.html"><a href="week-8-lecture.html#warm-up"><i class="fa fa-check"></i><b>14.1</b> Warm-up</a></li>
<li class="chapter" data-level="14.2" data-path="week-8-lecture.html"><a href="week-8-lecture.html#the-aims-of-modelling----a-discussion-of-shmueli-2010"><i class="fa fa-check"></i><b>14.2</b> The aims of modelling -- A discussion of Shmueli (2010)</a></li>
<li class="chapter" data-level="14.3" data-path="week-8-lecture.html"><a href="week-8-lecture.html#introduction-to-linear-models"><i class="fa fa-check"></i><b>14.3</b> Introduction to linear models</a></li>
<li class="chapter" data-level="14.4" data-path="week-8-lecture.html"><a href="week-8-lecture.html#linear-models-example-with-continuous-covariate"><i class="fa fa-check"></i><b>14.4</b> Linear models | example with continuous covariate</a></li>
<li class="chapter" data-level="14.5" data-path="week-8-lecture.html"><a href="week-8-lecture.html#resolving-overparameterization-using-contrasts"><i class="fa fa-check"></i><b>14.5</b> Resolving overparameterization using contrasts</a></li>
<li class="chapter" data-level="14.6" data-path="week-8-lecture.html"><a href="week-8-lecture.html#effect-codingtreatment-constrast"><i class="fa fa-check"></i><b>14.6</b> Effect coding/Treatment constrast</a></li>
<li class="chapter" data-level="14.7" data-path="week-8-lecture.html"><a href="week-8-lecture.html#helmert-contrasts"><i class="fa fa-check"></i><b>14.7</b> Helmert contrasts</a></li>
<li class="chapter" data-level="14.8" data-path="week-8-lecture.html"><a href="week-8-lecture.html#sum-to-zero-contrasts"><i class="fa fa-check"></i><b>14.8</b> Sum-to-zero contrasts</a></li>
<li class="chapter" data-level="14.9" data-path="week-8-lecture.html"><a href="week-8-lecture.html#polynomial-contrasts"><i class="fa fa-check"></i><b>14.9</b> Polynomial contrasts</a></li>
<li class="chapter" data-level="14.10" data-path="week-8-lecture.html"><a href="week-8-lecture.html#visualizing-hypotheses-for-different-coding-schemes"><i class="fa fa-check"></i><b>14.10</b> Visualizing hypotheses for different coding schemes</a></li>
<li class="chapter" data-level="14.11" data-path="week-8-lecture.html"><a href="week-8-lecture.html#orthogonal-vs.-non-orthogonal-contrasts"><i class="fa fa-check"></i><b>14.11</b> Orthogonal vs. Non-orthogonal contrasts</a></li>
<li class="chapter" data-level="14.12" data-path="week-8-lecture.html"><a href="week-8-lecture.html#error-structure-of-linear-models"><i class="fa fa-check"></i><b>14.12</b> Error structure of linear models</a><ul>
<li class="chapter" data-level="14.12.1" data-path="week-8-lecture.html"><a href="week-8-lecture.html#independence-of-errors"><i class="fa fa-check"></i><b>14.12.1</b> Independence of errors</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="week-8-lab.html"><a href="week-8-lab.html"><i class="fa fa-check"></i><b>15</b> Week 8 Lab</a><ul>
<li class="chapter" data-level="15.1" data-path="week-8-lab.html"><a href="week-8-lab.html#contrasts"><i class="fa fa-check"></i><b>15.1</b> Contrasts</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="week-9-lecture.html"><a href="week-9-lecture.html"><i class="fa fa-check"></i><b>16</b> Week 9 Lecture</a><ul>
<li class="chapter" data-level="16.1" data-path="week-9-lecture.html"><a href="week-9-lecture.html#correlation"><i class="fa fa-check"></i><b>16.1</b> Correlation</a></li>
<li class="chapter" data-level="16.2" data-path="week-9-lecture.html"><a href="week-9-lecture.html#hypothesis-testing---pearsons-r"><i class="fa fa-check"></i><b>16.2</b> Hypothesis testing - Pearson's <em>r</em></a></li>
<li class="chapter" data-level="16.3" data-path="week-9-lecture.html"><a href="week-9-lecture.html#fishers-z"><i class="fa fa-check"></i><b>16.3</b> Fisher's <span class="math inline">\(z\)</span></a></li>
<li class="chapter" data-level="16.4" data-path="week-9-lecture.html"><a href="week-9-lecture.html#regression"><i class="fa fa-check"></i><b>16.4</b> Regression</a><ul>
<li class="chapter" data-level="16.4.1" data-path="week-9-lecture.html"><a href="week-9-lecture.html#assumptions-of-regression"><i class="fa fa-check"></i><b>16.4.1</b> Assumptions of regression:</a></li>
</ul></li>
<li class="chapter" data-level="16.5" data-path="week-9-lecture.html"><a href="week-9-lecture.html#confidence-vs.-prediction-intervals"><i class="fa fa-check"></i><b>16.5</b> Confidence vs. Prediction intervals</a></li>
<li class="chapter" data-level="16.6" data-path="week-9-lecture.html"><a href="week-9-lecture.html#how-do-we-know-if-our-model-is-any-good"><i class="fa fa-check"></i><b>16.6</b> How do we know if our model is any good?</a></li>
<li class="chapter" data-level="16.7" data-path="week-9-lecture.html"><a href="week-9-lecture.html#robust-regression"><i class="fa fa-check"></i><b>16.7</b> Robust regression</a></li>
<li class="chapter" data-level="16.8" data-path="week-9-lecture.html"><a href="week-9-lecture.html#robust-regression-1"><i class="fa fa-check"></i><b>16.8</b> Robust regression</a></li>
<li class="chapter" data-level="16.9" data-path="week-9-lecture.html"><a href="week-9-lecture.html#type-i-and-type-ii-regression"><i class="fa fa-check"></i><b>16.9</b> Type I and Type II Regression</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="week-9-lab.html"><a href="week-9-lab.html"><i class="fa fa-check"></i><b>17</b> Week 9 Lab</a><ul>
<li class="chapter" data-level="17.1" data-path="week-9-lecture.html"><a href="week-9-lecture.html#correlation"><i class="fa fa-check"></i><b>17.1</b> Correlation</a></li>
<li class="chapter" data-level="17.2" data-path="week-9-lab.html"><a href="week-9-lab.html#linear-modelling"><i class="fa fa-check"></i><b>17.2</b> Linear modelling</a></li>
<li class="chapter" data-level="17.3" data-path="week-9-lab.html"><a href="week-9-lab.html#weighted-regression"><i class="fa fa-check"></i><b>17.3</b> Weighted regression</a></li>
<li class="chapter" data-level="17.4" data-path="week-9-lecture.html"><a href="week-9-lecture.html#robust-regression"><i class="fa fa-check"></i><b>17.4</b> Robust regression</a></li>
<li class="chapter" data-level="17.5" data-path="week-9-lab.html"><a href="week-9-lab.html#bootstrapping-standard-errors-for-robust-regression"><i class="fa fa-check"></i><b>17.5</b> Bootstrapping standard errors for robust regression</a></li>
<li class="chapter" data-level="17.6" data-path="week-9-lab.html"><a href="week-9-lab.html#type-i-vs.-type-ii-regression-the-smatr-package"><i class="fa fa-check"></i><b>17.6</b> Type I vs. Type II regression: The 'smatr' package</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="week-10-lecture.html"><a href="week-10-lecture.html"><i class="fa fa-check"></i><b>18</b> Week 10 Lecture</a><ul>
<li class="chapter" data-level="18.1" data-path="week-10-lecture.html"><a href="week-10-lecture.html#an-example"><i class="fa fa-check"></i><b>18.1</b> An example</a></li>
<li class="chapter" data-level="18.2" data-path="week-10-lecture.html"><a href="week-10-lecture.html#generalized-linear-models"><i class="fa fa-check"></i><b>18.2</b> Generalized linear models</a></li>
<li class="chapter" data-level="18.3" data-path="week-10-lecture.html"><a href="week-10-lecture.html#logistic-regression"><i class="fa fa-check"></i><b>18.3</b> Logistic regression</a></li>
<li class="chapter" data-level="18.4" data-path="week-10-lecture.html"><a href="week-10-lecture.html#fitting-a-glm"><i class="fa fa-check"></i><b>18.4</b> Fitting a GLM</a></li>
<li class="chapter" data-level="18.5" data-path="week-10-lecture.html"><a href="week-10-lecture.html#poisson-regression"><i class="fa fa-check"></i><b>18.5</b> Poisson regression</a></li>
<li class="chapter" data-level="18.6" data-path="week-10-lecture.html"><a href="week-10-lecture.html#deviance"><i class="fa fa-check"></i><b>18.6</b> Deviance</a></li>
<li class="chapter" data-level="18.7" data-path="week-10-lecture.html"><a href="week-10-lecture.html#other-methods----loess-splines-gams"><i class="fa fa-check"></i><b>18.7</b> Other methods -- LOESS, splines, GAMs</a><ul>
<li class="chapter" data-level="18.7.1" data-path="week-10-lecture.html"><a href="week-10-lecture.html#loess"><i class="fa fa-check"></i><b>18.7.1</b> LOESS</a></li>
<li class="chapter" data-level="18.7.2" data-path="week-10-lecture.html"><a href="week-10-lecture.html#splines"><i class="fa fa-check"></i><b>18.7.2</b> Splines</a></li>
<li class="chapter" data-level="18.7.3" data-path="week-10-lecture.html"><a href="week-10-lecture.html#gams"><i class="fa fa-check"></i><b>18.7.3</b> GAMs</a></li>
<li class="chapter" data-level="18.7.4" data-path="week-10-lecture.html"><a href="week-10-lecture.html#multiple-regression"><i class="fa fa-check"></i><b>18.7.4</b> Multiple regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="19" data-path="week-10-lab.html"><a href="week-10-lab.html"><i class="fa fa-check"></i><b>19</b> Week 10 Lab</a><ul>
<li class="chapter" data-level="19.1" data-path="week-10-lab.html"><a href="week-10-lab.html#discussion-of-challenger-analysis"><i class="fa fa-check"></i><b>19.1</b> Discussion of Challenger analysis</a><ul>
<li class="chapter" data-level="19.1.1" data-path="week-10-lab.html"><a href="week-10-lab.html#practice-fitting-models"><i class="fa fa-check"></i><b>19.1.1</b> Practice fitting models</a></li>
</ul></li>
<li class="chapter" data-level="19.2" data-path="week-10-lab.html"><a href="week-10-lab.html#weighted-linear-regression"><i class="fa fa-check"></i><b>19.2</b> Weighted linear regression</a></li>
<li class="chapter" data-level="19.3" data-path="week-10-lecture.html"><a href="week-10-lecture.html#logistic-regression"><i class="fa fa-check"></i><b>19.3</b> Logistic regression</a></li>
<li class="chapter" data-level="19.4" data-path="week-10-lecture.html"><a href="week-10-lecture.html#poisson-regression"><i class="fa fa-check"></i><b>19.4</b> Poisson regression</a></li>
<li class="chapter" data-level="19.5" data-path="week-10-lab.html"><a href="week-10-lab.html#getting-a-feel-for-deviance"><i class="fa fa-check"></i><b>19.5</b> Getting a feel for Deviance</a></li>
<li class="chapter" data-level="19.6" data-path="week-10-lab.html"><a href="week-10-lab.html#generalized-additive-models"><i class="fa fa-check"></i><b>19.6</b> Generalized Additive Models</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="week-11-lecture.html"><a href="week-11-lecture.html"><i class="fa fa-check"></i><b>20</b> Week 11 Lecture</a><ul>
<li class="chapter" data-level="20.0.1" data-path="week-11-lecture.html"><a href="week-11-lecture.html#variation-within-treatment-group"><i class="fa fa-check"></i><b>20.0.1</b> Variation within treatment group</a></li>
<li class="chapter" data-level="20.0.2" data-path="week-11-lecture.html"><a href="week-11-lecture.html#variation-among-treatment-group-means"><i class="fa fa-check"></i><b>20.0.2</b> Variation among treatment group means</a></li>
<li class="chapter" data-level="20.0.3" data-path="week-11-lecture.html"><a href="week-11-lecture.html#comparing-variance-components"><i class="fa fa-check"></i><b>20.0.3</b> Comparing variance components</a></li>
<li class="chapter" data-level="20.1" data-path="week-11-lecture.html"><a href="week-11-lecture.html#comparing-variance-components-1"><i class="fa fa-check"></i><b>20.1</b> Comparing variance components</a></li>
<li class="chapter" data-level="20.2" data-path="week-11-lecture.html"><a href="week-11-lecture.html#two-ways-to-estimate-variance"><i class="fa fa-check"></i><b>20.2</b> Two ways to estimate variance</a></li>
<li class="chapter" data-level="20.3" data-path="week-11-lecture.html"><a href="week-11-lecture.html#single-factor-anova"><i class="fa fa-check"></i><b>20.3</b> Single-factor ANOVA</a></li>
<li class="chapter" data-level="20.4" data-path="week-11-lecture.html"><a href="week-11-lecture.html#fixed-effects-vs.-random-effects"><i class="fa fa-check"></i><b>20.4</b> Fixed effects vs. random effects</a></li>
<li class="chapter" data-level="20.5" data-path="week-11-lecture.html"><a href="week-11-lecture.html#post-hoc-tests"><i class="fa fa-check"></i><b>20.5</b> Post-hoc tests</a><ul>
<li class="chapter" data-level="20.5.1" data-path="week-11-lecture.html"><a href="week-11-lecture.html#tukeys-hsd"><i class="fa fa-check"></i><b>20.5.1</b> Tukey's HSD</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="21" data-path="week-11-lab.html"><a href="week-11-lab.html"><i class="fa fa-check"></i><b>21</b> Week 11 Lab</a><ul>
<li class="chapter" data-level="21.1" data-path="week-11-lab.html"><a href="week-11-lab.html#rs-anova-functions"><i class="fa fa-check"></i><b>21.1</b> R's ANOVA functions</a></li>
<li class="chapter" data-level="21.2" data-path="week-11-lecture.html"><a href="week-11-lecture.html#single-factor-anova"><i class="fa fa-check"></i><b>21.2</b> Single-factor ANOVA</a></li>
<li class="chapter" data-level="21.3" data-path="week-11-lab.html"><a href="week-11-lab.html#follow-up-analyses-to-anova"><i class="fa fa-check"></i><b>21.3</b> Follow up analyses to ANOVA</a></li>
<li class="chapter" data-level="21.4" data-path="week-11-lab.html"><a href="week-11-lab.html#more-practice-model-i-anova"><i class="fa fa-check"></i><b>21.4</b> More practice: Model I ANOVA</a></li>
<li class="chapter" data-level="21.5" data-path="week-11-lab.html"><a href="week-11-lab.html#more-practice-brief-intro-to-doing-model-ii-anova-in-r"><i class="fa fa-check"></i><b>21.5</b> More practice: Brief intro to doing Model II ANOVA in R</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="week-12-lecture.html"><a href="week-12-lecture.html"><i class="fa fa-check"></i><b>22</b> Week 12 Lecture</a><ul>
<li class="chapter" data-level="22.1" data-path="week-12-lecture.html"><a href="week-12-lecture.html#review-anova-with-one-factor"><i class="fa fa-check"></i><b>22.1</b> Review: ANOVA with one factor</a></li>
<li class="chapter" data-level="22.2" data-path="week-12-lecture.html"><a href="week-12-lecture.html#anova-with-more-than-one-factor"><i class="fa fa-check"></i><b>22.2</b> ANOVA with more than one factor</a></li>
<li class="chapter" data-level="22.3" data-path="week-12-lecture.html"><a href="week-12-lecture.html#two-way-anova-factorial-designs"><i class="fa fa-check"></i><b>22.3</b> Two-way ANOVA factorial designs</a></li>
<li class="chapter" data-level="22.4" data-path="week-12-lecture.html"><a href="week-12-lecture.html#why-bother-with-random-effects"><i class="fa fa-check"></i><b>22.4</b> Why bother with random effects?</a></li>
<li class="chapter" data-level="22.5" data-path="week-12-lecture.html"><a href="week-12-lecture.html#mixed-models"><i class="fa fa-check"></i><b>22.5</b> Mixed models</a></li>
<li class="chapter" data-level="22.6" data-path="week-12-lecture.html"><a href="week-12-lecture.html#unbalanced-designs"><i class="fa fa-check"></i><b>22.6</b> Unbalanced designs</a></li>
<li class="chapter" data-level="22.7" data-path="week-12-lecture.html"><a href="week-12-lecture.html#unbalanced-design----different-sample-sizes"><i class="fa fa-check"></i><b>22.7</b> Unbalanced design -- Different sample sizes</a><ul>
<li class="chapter" data-level="22.7.1" data-path="week-12-lecture.html"><a href="week-12-lecture.html#type-i-sequential-sums-of-squares"><i class="fa fa-check"></i><b>22.7.1</b> Type I (sequential) sums of squares</a></li>
<li class="chapter" data-level="22.7.2" data-path="week-12-lecture.html"><a href="week-12-lecture.html#type-ii-hierarchical-sums-of-squares"><i class="fa fa-check"></i><b>22.7.2</b> Type II (hierarchical) sums of squares</a></li>
<li class="chapter" data-level="22.7.3" data-path="week-12-lecture.html"><a href="week-12-lecture.html#type-iii-marginal-sums-of-squares"><i class="fa fa-check"></i><b>22.7.3</b> Type III (marginal) sums of squares</a></li>
<li class="chapter" data-level="22.7.4" data-path="week-12-lecture.html"><a href="week-12-lecture.html#comparing-type-i-ii-and-iii-ss"><i class="fa fa-check"></i><b>22.7.4</b> Comparing type I, II, and III SS</a></li>
</ul></li>
<li class="chapter" data-level="22.8" data-path="week-12-lecture.html"><a href="week-12-lecture.html#unbalanced-design----missing-cells"><i class="fa fa-check"></i><b>22.8</b> Unbalanced design -- Missing cells</a></li>
<li class="chapter" data-level="22.9" data-path="week-12-lecture.html"><a href="week-12-lecture.html#two-factor-nested-anova"><i class="fa fa-check"></i><b>22.9</b> Two factor nested ANOVA</a><ul>
<li class="chapter" data-level="22.9.1" data-path="week-12-lecture.html"><a href="week-12-lecture.html#potential-issues-with-nested-designs"><i class="fa fa-check"></i><b>22.9.1</b> Potential issues with nested designs</a></li>
</ul></li>
<li class="chapter" data-level="22.10" data-path="week-12-lecture.html"><a href="week-12-lecture.html#experimental-design"><i class="fa fa-check"></i><b>22.10</b> Experimental design</a><ul>
<li class="chapter" data-level="22.10.1" data-path="week-12-lecture.html"><a href="week-12-lecture.html#completely-randomized-design"><i class="fa fa-check"></i><b>22.10.1</b> Completely randomized design</a></li>
<li class="chapter" data-level="22.10.2" data-path="week-12-lecture.html"><a href="week-12-lecture.html#randomized-block-design"><i class="fa fa-check"></i><b>22.10.2</b> Randomized block design</a></li>
<li class="chapter" data-level="22.10.3" data-path="week-12-lecture.html"><a href="week-12-lecture.html#latin-square-design"><i class="fa fa-check"></i><b>22.10.3</b> Latin square design</a></li>
<li class="chapter" data-level="22.10.4" data-path="week-12-lecture.html"><a href="week-12-lecture.html#split-plot-design"><i class="fa fa-check"></i><b>22.10.4</b> Split plot design</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="23" data-path="week-12-lab.html"><a href="week-12-lab.html"><i class="fa fa-check"></i><b>23</b> Week 12 Lab</a><ul>
<li class="chapter" data-level="23.1" data-path="week-12-lab.html"><a href="week-12-lab.html#example-1-two-way-factorial-anova-in-r"><i class="fa fa-check"></i><b>23.1</b> Example #1: Two-way factorial ANOVA in R</a></li>
<li class="chapter" data-level="23.2" data-path="week-12-lab.html"><a href="week-12-lab.html#example-2-nested-design"><i class="fa fa-check"></i><b>23.2</b> Example #2: Nested design</a></li>
<li class="chapter" data-level="23.3" data-path="week-12-lab.html"><a href="week-12-lab.html#example-3-nested-design"><i class="fa fa-check"></i><b>23.3</b> Example #3: Nested design</a></li>
<li class="chapter" data-level="23.4" data-path="week-12-lab.html"><a href="week-12-lab.html#example-4-randomized-block-design"><i class="fa fa-check"></i><b>23.4</b> Example #4: Randomized Block Design</a></li>
<li class="chapter" data-level="23.5" data-path="week-12-lab.html"><a href="week-12-lab.html#example-5-nested-design"><i class="fa fa-check"></i><b>23.5</b> Example #5: Nested design</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="week-13-lecture.html"><a href="week-13-lecture.html"><i class="fa fa-check"></i><b>24</b> Week 13 Lecture</a><ul>
<li class="chapter" data-level="24.1" data-path="week-13-lecture.html"><a href="week-13-lecture.html#model-criticism"><i class="fa fa-check"></i><b>24.1</b> Model criticism</a></li>
<li class="chapter" data-level="24.2" data-path="week-13-lecture.html"><a href="week-13-lecture.html#residuals"><i class="fa fa-check"></i><b>24.2</b> Residuals</a></li>
<li class="chapter" data-level="24.3" data-path="week-13-lecture.html"><a href="week-13-lecture.html#leverage"><i class="fa fa-check"></i><b>24.3</b> Leverage</a></li>
<li class="chapter" data-level="24.4" data-path="week-13-lecture.html"><a href="week-13-lecture.html#influence"><i class="fa fa-check"></i><b>24.4</b> Influence</a></li>
<li class="chapter" data-level="24.5" data-path="week-13-lecture.html"><a href="week-13-lecture.html#comparing-residuals-leverage-and-influence"><i class="fa fa-check"></i><b>24.5</b> Comparing residuals, leverage, and influence</a></li>
<li class="chapter" data-level="24.6" data-path="week-13-lecture.html"><a href="week-13-lecture.html#residuals-for-glms"><i class="fa fa-check"></i><b>24.6</b> Residuals for GLMs</a></li>
<li class="chapter" data-level="24.7" data-path="week-13-lecture.html"><a href="week-13-lecture.html#model-selection-vs.-model-criticism"><i class="fa fa-check"></i><b>24.7</b> Model selection vs. model criticism</a></li>
<li class="chapter" data-level="24.8" data-path="week-13-lecture.html"><a href="week-13-lecture.html#comparing-two-models"><i class="fa fa-check"></i><b>24.8</b> Comparing two models</a><ul>
<li class="chapter" data-level="24.8.1" data-path="week-13-lecture.html"><a href="week-13-lecture.html#nested-or-not"><i class="fa fa-check"></i><b>24.8.1</b> Nested or not?</a></li>
<li class="chapter" data-level="24.8.2" data-path="week-13-lecture.html"><a href="week-13-lecture.html#likelihood-ratio-test-lrt"><i class="fa fa-check"></i><b>24.8.2</b> Likelihood Ratio Test (LRT)</a></li>
<li class="chapter" data-level="24.8.3" data-path="week-13-lecture.html"><a href="week-13-lecture.html#akaikes-information-criterion-aic"><i class="fa fa-check"></i><b>24.8.3</b> Akaike's Information Criterion (AIC)</a></li>
<li class="chapter" data-level="24.8.4" data-path="week-13-lecture.html"><a href="week-13-lecture.html#bayesian-information-criterion-bic"><i class="fa fa-check"></i><b>24.8.4</b> Bayesian Information Criterion (BIC)</a></li>
<li class="chapter" data-level="24.8.5" data-path="week-13-lecture.html"><a href="week-13-lecture.html#comparing-lrt-and-aicbic"><i class="fa fa-check"></i><b>24.8.5</b> Comparing LRT and AIC/BIC</a></li>
</ul></li>
<li class="chapter" data-level="24.9" data-path="week-13-lecture.html"><a href="week-13-lecture.html#model-weighting"><i class="fa fa-check"></i><b>24.9</b> Model weighting</a></li>
<li class="chapter" data-level="24.10" data-path="week-13-lecture.html"><a href="week-13-lecture.html#stepwise-regression"><i class="fa fa-check"></i><b>24.10</b> Stepwise regression</a><ul>
<li class="chapter" data-level="24.10.1" data-path="week-13-lecture.html"><a href="week-13-lecture.html#criticism-of-stepwise-regression"><i class="fa fa-check"></i><b>24.10.1</b> Criticism of stepwise regression</a></li>
<li class="chapter" data-level="24.10.2" data-path="week-13-lecture.html"><a href="week-13-lecture.html#criticism-of-data-dredging"><i class="fa fa-check"></i><b>24.10.2</b> Criticism of data dredging</a></li>
<li class="chapter" data-level="24.10.3" data-path="week-13-lecture.html"><a href="week-13-lecture.html#final-thoughts-on-model-selection"><i class="fa fa-check"></i><b>24.10.3</b> Final thoughts on model selection</a></li>
</ul></li>
<li class="chapter" data-level="24.11" data-path="week-13-lecture.html"><a href="week-13-lecture.html#week-13-faq"><i class="fa fa-check"></i><b>24.11</b> Week 13 FAQ</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="week-13-lab.html"><a href="week-13-lab.html"><i class="fa fa-check"></i><b>25</b> Week 13 Lab</a><ul>
<li class="chapter" data-level="25.1" data-path="week-13-lab.html"><a href="week-13-lab.html#part-1-model-selection-model-comparison"><i class="fa fa-check"></i><b>25.1</b> Part 1: Model selection / model comparison</a></li>
<li class="chapter" data-level="25.2" data-path="week-13-lab.html"><a href="week-13-lab.html#model-selection-via-step-wise-regression"><i class="fa fa-check"></i><b>25.2</b> Model selection via step-wise regression</a></li>
<li class="chapter" data-level="25.3" data-path="week-13-lab.html"><a href="week-13-lab.html#part-2-model-criticism"><i class="fa fa-check"></i><b>25.3</b> Part 2: Model criticism</a></li>
</ul></li>
<li class="chapter" data-level="26" data-path="week-14-lecture.html"><a href="week-14-lecture.html"><i class="fa fa-check"></i><b>26</b> Week 14 Lecture</a><ul>
<li class="chapter" data-level="26.1" data-path="week-14-lecture.html"><a href="week-14-lecture.html#what-does-multivariate-mean"><i class="fa fa-check"></i><b>26.1</b> What does 'multivariate' mean?</a></li>
<li class="chapter" data-level="26.2" data-path="week-14-lecture.html"><a href="week-14-lecture.html#multivariate-associations"><i class="fa fa-check"></i><b>26.2</b> Multivariate associations</a></li>
<li class="chapter" data-level="26.3" data-path="week-14-lecture.html"><a href="week-14-lecture.html#model-criticism-for-multivariate-analyses"><i class="fa fa-check"></i><b>26.3</b> Model criticism for multivariate analyses</a><ul>
<li class="chapter" data-level="26.3.1" data-path="week-14-lecture.html"><a href="week-14-lecture.html#transforming-your-data"><i class="fa fa-check"></i><b>26.3.1</b> Transforming your data</a></li>
</ul></li>
<li class="chapter" data-level="26.4" data-path="week-14-lecture.html"><a href="week-14-lecture.html#standardizing-your-data"><i class="fa fa-check"></i><b>26.4</b> Standardizing your data</a></li>
<li class="chapter" data-level="26.5" data-path="week-14-lecture.html"><a href="week-14-lecture.html#multivariate-outliers"><i class="fa fa-check"></i><b>26.5</b> Multivariate outliers</a></li>
<li class="chapter" data-level="26.6" data-path="week-14-lecture.html"><a href="week-14-lecture.html#brief-overview-of-multivariate-analyses"><i class="fa fa-check"></i><b>26.6</b> Brief overview of multivariate analyses</a></li>
<li class="chapter" data-level="26.7" data-path="week-14-lecture.html"><a href="week-14-lecture.html#manova-and-dfa"><i class="fa fa-check"></i><b>26.7</b> MANOVA and DFA</a></li>
<li class="chapter" data-level="26.8" data-path="week-14-lecture.html"><a href="week-14-lecture.html#scaling-or-ordination-techniques"><i class="fa fa-check"></i><b>26.8</b> Scaling or ordination techniques</a></li>
<li class="chapter" data-level="26.9" data-path="week-14-lecture.html"><a href="week-14-lecture.html#principal-components-analysis-pca"><i class="fa fa-check"></i><b>26.9</b> Principal components analysis (PCA)</a></li>
<li class="chapter" data-level="26.10" data-path="week-14-lecture.html"><a href="week-14-lecture.html#principal-components-analysis-pca-1"><i class="fa fa-check"></i><b>26.10</b> Principal components analysis (PCA)</a></li>
<li class="chapter" data-level="26.11" data-path="week-14-lecture.html"><a href="week-14-lecture.html#pca-in-r"><i class="fa fa-check"></i><b>26.11</b> PCA in R</a></li>
<li class="chapter" data-level="26.12" data-path="week-14-lecture.html"><a href="week-14-lecture.html#missing-data"><i class="fa fa-check"></i><b>26.12</b> Missing data</a></li>
<li class="chapter" data-level="26.13" data-path="week-14-lecture.html"><a href="week-14-lecture.html#imputing-missing-data"><i class="fa fa-check"></i><b>26.13</b> Imputing missing data</a></li>
</ul></li>
<li class="chapter" data-level="27" data-path="week-14-lab.html"><a href="week-14-lab.html"><i class="fa fa-check"></i><b>27</b> Week 14 Lab</a><ul>
<li class="chapter" data-level="27.1" data-path="week-14-lab.html"><a href="week-14-lab.html#missing-at-random---practice-with-glms"><i class="fa fa-check"></i><b>27.1</b> Missing at random - practice with GLMs</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Biometry Lecture and Lab Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="week-5-lab" class="section level1">
<h1><span class="header-section-number">10</span> Week 5 Lab</h1>
<p>To learn more about R`s functions for doing the hypothesis tests introduced on Tuesday, we will simulate some data. Simulating data with known properties is always the best way of exploring a statistical test before applying it to your own data. Although we could simulate data using the &quot;rnorm&quot; function, we will use the &quot;mvrnorm&quot; function from the MASS package so we can consider the role of correlation between the two samples.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(MASS)
data&lt;-<span class="kw">mvrnorm</span>(<span class="dt">n=</span><span class="dv">100</span>,<span class="kw">c</span>(<span class="dv">1</span>,<span class="fl">1.2</span>),<span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">3</span>),<span class="dv">2</span>,<span class="dv">2</span>))</code></pre></div>
<p>The call here is similar to the one used for rnorm except now we are drawing two samples at the same time with the following properties:</p>
<p><span class="math display">\[
\left[ \begin{array}{c}
X_{A}  \\
X_{B}  \end{array} \right] \sim N\left(\left(\begin{array}{c}
\mu_{A}  \\
\mu_{B}  \end{array}\right),\left(\begin{array}{cc}
Cov(A,A) &amp; Cov(A,B)   \\
Cov(A,B) &amp; Cov(B,B) \end{array}\right)\right)
\]</span></p>
<p>Note that the variances have to be non-negative, and because Cov(A,B)=Cov(B,A), the covariance matrix has to be symmetric. (CAREFUL: mvrnorm does not enforce this!)</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sample.a&lt;-data[,<span class="dv">1</span>]
sample.b&lt;-data[,<span class="dv">2</span>]
<span class="kw">plot</span>(sample.a,sample.b)</code></pre></div>
<p><img src="Week-5-lab_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>Notice that because Cov<span class="math inline">\((X_{A},X_{B})\)</span>=0, the scatterplot has no trend.</p>
<p>The t-test will answer the question, &quot;Can we reject the null hypothesis that these two populations have the same mean&quot;?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">t.test</span>(sample.a,sample.b)</code></pre></div>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  sample.a and sample.b
## t = -1.7946, df = 193.7, p-value = 0.07428
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -0.96936602  0.04573266
## sample estimates:
## mean of x mean of y 
## 0.5761132 1.0379299</code></pre>
<p>Notice that the degrees of freedom need not necessarily be an integer. R assumes unequal variances so we have to use the more complicated formula for the degrees of freedom. (In this situation, the number of degrees of freedom is not very intuitive; however, if you have <span class="math inline">\(s_{A}^{2}=s_{B}^{2}\)</span> and <span class="math inline">\(n_{A}=n_{B}\)</span>, then the formula for the d.o.f. simplifies to 2n-2 which is what you would expect. )</p>
<p>Because we are simulating data, everyone's output is going to look slightly different. In fact, some of us may get p&lt;0.05 and others p&gt;0.05. Here we are looking for a relatively small difference (compared to the standard deviations) with only 100 random draws. We would get more consistent results if we had larger sample sizes, larger mean differences, or smaller standard deviations. This touches on the very important question of statistical POWER, which we will discuss next week.</p>
<p><strong><span style="color: green;">Checkpoint #1: The output of t.test includes 7 quantities - can you reproduce all 7 quantities based on what we learned on Tuesday?</span></strong></p>
<p>The default in R is to assume a two-sided test. In other words, by default, R tests</p>
<p><span class="math display">\[
H_{0}:\mu_{1}=\mu_{2},                                                
H_{A}:\mu_{1}\neq\mu_{2}
\]</span></p>
<p>Alternatively, you can force R to do a one-sided test, either</p>
<p><span class="math display">\[
H_{A}:\mu_{1}&gt;\mu_{2}
\]</span></p>
<p>or</p>
<p><span class="math display">\[
H_{A}:\mu_{1}&lt;\mu_{2}
\]</span></p>
<p>by using the options</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">t.test</span>(sample.a,sample.b,<span class="dt">alternative=</span><span class="st">&quot;greater&quot;</span>)</code></pre></div>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  sample.a and sample.b
## t = -1.7946, df = 193.7, p-value = 0.9629
## alternative hypothesis: true difference in means is greater than 0
## 95 percent confidence interval:
##  -0.8871384        Inf
## sample estimates:
## mean of x mean of y 
## 0.5761132 1.0379299</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">t.test</span>(sample.a,sample.b,<span class="dt">alternative=</span><span class="st">&quot;less&quot;</span>)</code></pre></div>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  sample.a and sample.b
## t = -1.7946, df = 193.7, p-value = 0.03714
## alternative hypothesis: true difference in means is less than 0
## 95 percent confidence interval:
##         -Inf -0.03649496
## sample estimates:
## mean of x mean of y 
## 0.5761132 1.0379299</code></pre>
<p>Try both of these options and see how the p-value and the confidence intervals change. Make sure you understand why they make sense. Remember, if you are going to use a one-tailed test, you should be prepared to accept that a large difference opposite to what was expected is pure random chance.</p>
<p>Spend some time going back and experimenting with different sets of random variables. Make the means more or less different. Change the variances. Make sure you understand why the t-test results change as you alter the data. <strong><span style="color: green;">Checkpoint #2: What happens if you make the sample sizes smaller or larger?</span></strong></p>
<p>Now let's see what happens when we simulate a new dataset with correlations between the two samples.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">data&lt;-<span class="kw">mvrnorm</span>(<span class="dt">n=</span><span class="dv">100</span>,<span class="kw">c</span>(<span class="dv">1</span>,<span class="fl">1.2</span>),<span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">3</span>),<span class="dv">2</span>,<span class="dv">2</span>))
sample.a&lt;-data[,<span class="dv">1</span>]
sample.b&lt;-data[,<span class="dv">2</span>]
<span class="kw">plot</span>(sample.a,sample.b)</code></pre></div>
<p><img src="Week-5-lab_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>Notice that now the two samples are positively correlated.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">t.test</span>(sample.a,sample.b,<span class="dt">paired=</span>T)</code></pre></div>
<pre><code>## 
##  Paired t-test
## 
## data:  sample.a and sample.b
## t = 0.089288, df = 99, p-value = 0.929
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -0.2618037  0.2864759
## sample estimates:
## mean of the differences 
##              0.01233611</code></pre>
<p>Compare this with</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">t.test</span>(sample.a,sample.b,<span class="dt">paired=</span>F)</code></pre></div>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  sample.a and sample.b
## t = 0.053625, df = 193.1, p-value = 0.9573
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -0.4413843  0.4660565
## sample estimates:
## mean of x mean of y 
##  1.031639  1.019303</code></pre>
<p>which is the same as</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">t.test</span>(sample.a,sample.b)</code></pre></div>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  sample.a and sample.b
## t = 0.053625, df = 193.1, p-value = 0.9573
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -0.4413843  0.4660565
## sample estimates:
## mean of x mean of y 
##  1.031639  1.019303</code></pre>
<p>since the default is to assume unpaired samples.</p>
<p>To convince ourselves of the formulas we learned for paired t-tests, we will define a new variable</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">z&lt;-sample.a-sample.b
<span class="kw">mean</span>(z)</code></pre></div>
<pre><code>## [1] 0.01233611</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">var</span>(z)</code></pre></div>
<pre><code>## [1] 1.908824</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">var</span>(sample.a)+<span class="kw">var</span>(sample.b)-<span class="dv">2</span>*<span class="kw">cov</span>(sample.a,sample.b)</code></pre></div>
<pre><code>## [1] 1.908824</code></pre>
<p>Spend some time going back and experimenting with different sets of random variables. In particular, change the covariances (makign sure that the covariance matrix is symmetric and the variances positive). Make sure you understand why the t-test results change as you alter the data. <strong><span style="color: green;">Checkpoint #3: What happens to the t-test results when you make the correlation weak?</span></strong></p>
<div id="f-test" class="section level2">
<h2><span class="header-section-number">10.1</span> F-test</h2>
<p>One of the conditions for using the t-test is that the variances of the two samples are the same. One way of determining if the two variances are the same is the F-test. To review what we learned on Tuesday, under the null hypothesis that two populations have the same <span class="math inline">\(\sigma^{2}\)</span>, the ratio of their sample variances will follow an F-distribution</p>
<p><span class="math display">\[
F_{s}=\frac{s_{A}^{2}}{s_{B}^{2}}
\]</span></p>
<p>If the value of the test statistic would be unusual for the F-distribution, we would reject the null hypothesis. (In other words, such a ratio of variances would be unlikely to occur by chance if the two variances were in fact the same.)</p>
<p>As with the t-test, we can do a one-tailed or two-tailed F-test. The one-tailed F-test presumes that we want to test whether one variance is significantly larger than another variance, i.e.</p>
<p><span class="math display">\[
H_{0}: \sigma_{A}^{2} = \sigma_{B}^{2}
\]</span></p>
<p><span class="math display">\[
H_{\mbox{A implied}}: \sigma_{A}^{2} &gt; \sigma_{B}^{2}
\]</span></p>
<p>For a one-tailed test, we would assume that if the data were to show that</p>
<p><span class="math display">\[
s_{B}^{2} &gt; s_{A}^{2}
\]</span></p>
<p>than we would chalk that up to random chance and we would NOT interpret that as</p>
<p><span class="math display">\[
\sigma_{B}^{2} &gt; \sigma_{A}^{2}.
\]</span></p>
<p>If we are doing a one-tailed F-test where we are only interested in <span class="math inline">\(\sigma_{A}^{2} &gt; \sigma_{B}^{2}\)</span>, then our test statistic is <span class="math inline">\(\frac{s_{A}^{2}}{s_{B}^{2}}\)</span> (regardless of which is bigger), and we would set our critical value so that <span class="math inline">\(\alpha\)</span> of the distribution of the test statistic would fall into the right-hand tail of the F-distribution. We would then compare our test statistic <span class="math inline">\(T^{*}\)</span> to the value of the (1-<span class="math inline">\(\alpha\)</span>) quantile of the F-distribution, and the question becomes</p>
<p><span class="math display">\[
\frac{s_{A}^{2}}{s_{B}^{2}} &gt; F_{[1-\alpha](n-1,m-1)}?
\]</span></p>
<p>However, if we are open to either</p>
<p><span class="math display">\[
H_{\mbox{A implied}}: \sigma_{A}^{2} &gt; \sigma_{B}^{2}
\]</span></p>
<p>or</p>
<p><span class="math display">\[
H_{\mbox{A implied}}: \sigma_{A}^{2} &lt; \sigma_{B}^{2}
\]</span></p>
<p>then we want to do a two-tailed test. In this case, by tradition and to make life simpler, the larger sample variance always goes in the numerator, and we check this value against the critical value obtained by putting only 2.5% in the right hand tail. Now we are asking the question</p>
<p><span class="math display">\[
\frac{s_{A}^{2}}{s_{B}^{2}} &gt; F_{[1-\frac{\alpha}{2}](n-1,m-1)}?
\]</span></p>
<p>In other words, because the F-distribution is always non-negative, it is easier to put the larger sample variance in the numerator to create a test statistic <span class="math inline">\(T^{*}&gt;1\)</span> and then compare that to <span class="math inline">\(F_{[1-\frac{\alpha}{2}](n-1,m-1)}\)</span> than it is to worry about computing both critical values (one for a ratio <span class="math inline">\(&lt;1\)</span> and a separate one for a ratio <span class="math inline">\(&gt;1\)</span>).</p>
<p>Let's explore this by simulating some data</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sample.a&lt;-<span class="kw">rnorm</span>(<span class="dv">100</span>,<span class="dt">mean=</span><span class="dv">5</span>,<span class="dt">sd=</span><span class="dv">4</span>)
sample.b&lt;-<span class="kw">rnorm</span>(<span class="dv">100</span>,<span class="dt">mean=</span><span class="dv">2</span>,<span class="dt">sd=</span><span class="dv">3</span>)</code></pre></div>
<p>Lets assume we are only interested in the implied alternative hypothesis</p>
<p><span class="math display">\[
H_{A implied}: \sigma_{A}^{2} &gt; \sigma_{B}^{2}
\]</span></p>
<p>We can calculate this one-tailed F-test by hand first</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">var.A&lt;-<span class="kw">var</span>(sample.a)
var.B&lt;-<span class="kw">var</span>(sample.b)
F.ratio&lt;-var.A/var.B 
F.ratio</code></pre></div>
<pre><code>## [1] 1.430759</code></pre>
<p>Note that we didn't check that var.A was actually bigger than var.B. Because we are only interested in a one-tailed test, we want var.A in the numerator and we will compare that to the right-hand side of the F-distribution.</p>
<p>Now we know what the F-ratio of our simulated data is, but we don't know what the critical value of the test statistic is for the ratio to be considered &quot;significant&quot;.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qf</span>(<span class="dt">p=</span><span class="fl">0.95</span>,<span class="dt">df1=</span><span class="dv">99</span>,<span class="dt">df2=</span><span class="dv">99</span>)</code></pre></div>
<pre><code>## [1] 1.394061</code></pre>
<p><strong><span style="color: green;">Checkpoint #4: Do you understand why df1=99 and df2=99?</span></strong></p>
<p>We see that the F-ratio for our data is greater than the critical value for the 95th percentile. Therefore we can say that the two samples are unlikely to come from populations from the same variance. We can also flip the question around to ask: &quot;What is the probability of obtaining this F-ratio if the null hypothesis were true?&quot;</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="dv">1</span>-<span class="kw">pf</span>(F.ratio,<span class="dt">df1=</span><span class="dv">99</span>,<span class="dt">df2=</span><span class="dv">99</span>)</code></pre></div>
<pre><code>## [1] 0.03812701</code></pre>
<p>and we see that it is small (&lt;0.05).</p>
<p>Remember, pf gives the cumulative probability of getting any value less than or equal to the percentile being queried, so 1-pf gives the probability of getting a value as or more extreme.</p>
<p>R gives us an easier way to test equality of variances (the F test):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">var.test</span>(sample.a,sample.b)</code></pre></div>
<pre><code>## 
##  F test to compare two variances
## 
## data:  sample.a and sample.b
## F = 1.4308, num df = 99, denom df = 99, p-value = 0.07625
## alternative hypothesis: true ratio of variances is not equal to 1
## 95 percent confidence interval:
##  0.9626742 2.1264422
## sample estimates:
## ratio of variances 
##           1.430759</code></pre>
<p>The output of var.test includes 7 quantities - make sure you can calculate each and every one of these quantities.</p>
<p>How did R arrive at its p-value? Notice that it is twice what we calculated when doing the F-test by hand. The reason is that R's default is to do a two-sided test, and we only calculated the probability that you got a ratio as big or larger than the <span class="math inline">\(F^{*}\)</span>. You can get the two-tailed probability by either doubling that value or by adding the probability that you would get an F-ratio smaller than 1/<span class="math inline">\(F^{*}\)</span>.</p>
<p>Notice also that to get the confidence intervals on the F-ratio, you multiply by the quantiles of the F-distribution.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qf</span>(<span class="fl">0.025</span>,<span class="dt">df1=</span><span class="dv">99</span>,<span class="dt">df2=</span><span class="dv">99</span>)*F.ratio</code></pre></div>
<pre><code>## [1] 0.9626742</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qf</span>(<span class="fl">0.975</span>,<span class="dt">df1=</span><span class="dv">99</span>,<span class="dt">df2=</span><span class="dv">99</span>)*F.ratio</code></pre></div>
<pre><code>## [1] 2.126442</code></pre>
</div>
<div id="comparing-two-proportions" class="section level2">
<h2><span class="header-section-number">10.2</span> Comparing two proportions</h2>
<p>On Tuesday I introduced two different test statistics that are frequently used to test a binomial proportion.</p>
<p>The Wald test is based on the idea that near the minimum of the likelihood function (in the vicinity of the MLE), deviations from the MLE will be normally distributed. Therefore, the Wald test is a very general statement</p>
<p><span class="math display">\[
\frac{\hat{\theta}-\theta_{0}}{se(\hat{\theta})}\sim N(0,1)
\]</span></p>
<p>which we can apply in this specific case as</p>
<p><span class="math display">\[
\frac{\hat{p}-\theta_{0}}{\sqrt{\frac{\hat{p}(1-\hat{p})}{n}}}\sim N(0,1) 
\]</span></p>
<p>The 'score' test uses the value of the binomial proportion <em>under the null hypothesis</em> in place of the empirical probability in estimating the standard error in the denominator.</p>
<p><span class="math display">\[
\frac{\hat{p}-\theta_{0}}{\sqrt{\frac{\theta_{0}(1-\theta_{0})}{n}}}\sim N(0,1) 
\]</span></p>
<p>We can easily invert the Wald test to arrive at the following confidence interval for p:</p>
<p><span class="math display">\[
P\left(\hat{p}-z_{1-(\alpha/2)}\sqrt{\frac{\hat{p}(1-\hat{p})}{n}}\leq \theta \leq \hat{p}+z_{1-(\alpha/2)}\sqrt{\frac{\hat{p}(1-\hat{p})}{n}}\right)=1-\alpha
\]</span></p>
<p>However, if you look at the score test, you see that it is quite difficult to invert and isolate <span class="math inline">\(\hat{p}\)</span>. This is why the confidence intervals for the score test are so complicated. We will not go into more detail here because in practice you would not be calculating it by hand anyways (but the general idea behind inverting the test to get a CI should be clear!).</p>
<p>We will now explore R's functions for estimating the probability <span class="math inline">\(\hat{p}\)</span> (and its confidence interval) for a binomial distribution.</p>
<p>First we simulate some data to work with</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">heads&lt;-<span class="kw">rbinom</span>(<span class="dv">1</span>,<span class="dt">size=</span><span class="dv">100</span>,<span class="dt">prob=</span><span class="fl">0.5</span>)
heads</code></pre></div>
<pre><code>## [1] 48</code></pre>
<p>Now we will use R's function for the proportion test</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">prop.test</span>(heads,<span class="dv">100</span>) <span class="co">#continuity correction true by default</span></code></pre></div>
<pre><code>## 
##  1-sample proportions test with continuity correction
## 
## data:  heads out of 100, null probability 0.5
## X-squared = 0.09, df = 1, p-value = 0.7642
## alternative hypothesis: true p is not equal to 0.5
## 95 percent confidence interval:
##  0.3798722 0.5816817
## sample estimates:
##    p 
## 0.48</code></pre>
<p>The continuity correction is used because we are approximating a discrete distribution (the binomial) with its normal approximation. While generally recommended, it rarely makes a large difference and for transparency I suggest we turn it off.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">prop.test</span>(heads,<span class="dv">100</span>,<span class="dt">correct=</span><span class="ot">FALSE</span>)</code></pre></div>
<pre><code>## 
##  1-sample proportions test without continuity correction
## 
## data:  heads out of 100, null probability 0.5
## X-squared = 0.16, df = 1, p-value = 0.6892
## alternative hypothesis: true p is not equal to 0.5
## 95 percent confidence interval:
##  0.3846455 0.5768342
## sample estimates:
##    p 
## 0.48</code></pre>
<p>To make sure we understand this, let's look at the help file for prop.test</p>
<pre><code>?prop.test</code></pre>
<p>There are a couple of additional pieces of information you need to understand and/or reconstruct the output for prop.test, and unfortunately the help file for this function is fairly unhelpful.</p>
<ul>
<li>R uses the <strong>score test</strong>, so the confidence interval computed by R won't be quite the same as what we would calculate using the Wald test method introduced in lecture. (But they will be close.)</li>
<li>The test statistic that prop.test reports is the square of the score test statistic introduced in lecture</li>
</ul>
<p><span class="math display">\[
\left[\frac{\hat{p}-\theta_{0}}{\sqrt{\frac{\theta_{0}(1-\theta_{0})}{n}}}\right]^{2}
\]</span></p>
<p>This is fine. Remember that there is no &quot;correct&quot; test statistic. The only challenge here is that we need to find the distribution of the test statistic under the null hypothesis. In this case, we remember from Week 3 that if we have a random variable X</p>
<p><span class="math display">\[
X \sim N(0,1)
\]</span></p>
<p>then the square of X</p>
<p><span class="math display">\[
X^{2} \sim \chi^{2}_{1}
\]</span></p>
<p>Therefore, the test statistic output by prop.test (assuming <span class="math inline">\(H_{0}: \theta_{0}=0.5\)</span>) is</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n&lt;-<span class="dv">100</span>
theta0&lt;-<span class="fl">0.5</span>
test.statistic&lt;-(((heads/n)-theta0)/(<span class="kw">sqrt</span>(theta0*theta0/n)))^<span class="dv">2</span>
test.statistic</code></pre></div>
<pre><code>## [1] 0.16</code></pre>
<p>and the p-value is given by</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="dv">1</span>-<span class="kw">pchisq</span>(test.statistic,<span class="dt">df=</span><span class="dv">1</span>)</code></pre></div>
<pre><code>## [1] 0.6891565</code></pre>
<p>Notice that because <span class="math inline">\(X^{2}\)</span> is always positive, the test is a one-tailed test (&quot;extreme&quot; values of this test statistic are always large and <strong>positive</strong>). We could get the same result comparing the original (non-squared) test statistic against the Standard Normal, as long as you make sure to do the two-tailed test (&quot;extreme&quot; for X includes large and positive <strong>and</strong> large and negative).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="dv">2</span>*(<span class="dv">1</span>-<span class="kw">pnorm</span>(<span class="kw">sqrt</span>(test.statistic)))</code></pre></div>
<pre><code>## [1] 0.6891565</code></pre>
<ul>
<li>Notice in the helpfile that R will return a clipped version of the confidence interval, so the confidence interval is bounded [0,1].</li>
</ul>
<p>There is another function in R called &quot;binom.test&quot; which does an &quot;exact test&quot; based on the actual binomial distribution, rather than the normal approximation to it we introduced on Tuesday. Notice that there is no continuity correction because we are not approximating the binomial distribution with the normal distribution.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">binom.test</span>(heads,<span class="dv">100</span>)</code></pre></div>
<pre><code>## 
##  Exact binomial test
## 
## data:  heads and 100
## number of successes = 48, number of trials = 100, p-value = 0.7644
## alternative hypothesis: true probability of success is not equal to 0.5
## 95 percent confidence interval:
##  0.3790055 0.5822102
## sample estimates:
## probability of success 
##                   0.48</code></pre>
<p>One of the caveats in using the Wald method for binomial proportion is that it can give unrealistic values for the CIs when the underlying probability is very close to either 0 or 1 (especially if the sample size is small).</p>
<p>To see this, lets calculate the Wald confidence intervals by hand assuming we got 2 successes out of 100 trials.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">p.hat&lt;-<span class="dv">2</span>/<span class="dv">100</span>
n&lt;-<span class="dv">100</span>
se&lt;-<span class="kw">sqrt</span>(p.hat*(<span class="dv">1</span>-p.hat)/n)
z&lt;-<span class="kw">qnorm</span>(<span class="fl">0.975</span>)
LL&lt;-p.hat-se*z
UL&lt;-p.hat+se*z
LL</code></pre></div>
<pre><code>## [1] -0.007439496</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">UL</code></pre></div>
<pre><code>## [1] 0.0474395</code></pre>
<p>The lower limit of the CI is actually negative which makes no sense since the binomial proportion is bounded [0,1]. We can clip the CI to have a LL of zero, but there is no guarentee that the &quot;coverage&quot; of the CI contains 95%. To see whether this is true, lets simulate multiple datasets with a true underlying probability of 0.02, calculate confidence intervals from those random samples, and see what proportion of the CIs actually contain the true value (0.02).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">data&lt;-<span class="kw">rbinom</span>(<span class="dt">n=</span><span class="dv">1000</span>,<span class="dt">size=</span><span class="dv">100</span>,<span class="dt">prob=</span><span class="fl">0.02</span>) <span class="co">#draw 1000 random datasets-100 trials each</span>
p.hat&lt;-data/<span class="dv">100</span> <span class="co">#vector of empirical probabilities p.hat</span>
n&lt;-<span class="dv">100</span>
se&lt;-<span class="kw">sqrt</span>(p.hat*(<span class="dv">1</span>-p.hat)/n)
z&lt;-<span class="kw">qnorm</span>(<span class="fl">0.975</span>)
LL&lt;-p.hat-se*z <span class="co">#vector of LLs</span>
UL&lt;-p.hat+se*z <span class="co">#vector of UL</span>
<span class="kw">sum</span>(<span class="kw">as.numeric</span>((<span class="fl">0.02</span>&lt;=UL)&amp;(<span class="fl">0.02</span>&gt;=LL)))/<span class="dv">1000</span> <span class="co">#proportion of times the CI include the true value</span></code></pre></div>
<pre><code>## [1] 0.852</code></pre>
<p>We see that the CIs are actually too narrow! The Wald test is commonly used but as we have demonstrated, it is not very good in practice. R and its packages make it easy to get better CIs for a binomial proportion and in real analysis, you should use these more sophisticated methods.</p>
<p>One of the better functions for doing tests of binomial proportions is provided by Dr. Dirk Enzmann. He has posted a function online that calculates 11 different versions of the binomial confidence interval.</p>
<p>I have pasted the function here:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Calculate confidence intervals for a single proportion according to one of 11</span>
<span class="co"># methods (default: log likelihood ratio) for a level of alpha (default: .05)</span>
<span class="co"># and with significant decimals (default: 3) given a proportion and the sample</span>
<span class="co"># size (see: Newcombe, 1998; Brown et al., 2001). Except for the asymptotic and</span>
<span class="co"># Agresti&#39;s methods all methods can yield asymmetric intervals around p:</span>
<span class="co">#</span>
<span class="co"># Methods are:</span>
<span class="co"># &#39;likelihood&#39;    : log likelihood ratio interval (default)</span>
<span class="co"># &#39;asymptotic&#39;    : simple &quot;classical text book&quot; or Wald method interval</span>
<span class="co"># &#39;asymptotic.cc  : asymptotic with Yates&#39; continuity correction</span>
<span class="co"># &#39;score&#39;         : score or Wilson method interval (= prop.test(...,correct=F),</span>
<span class="co">#                   i.e. without Yates&#39; continuity correction)</span>
<span class="co"># &#39;score.cc&#39;      : score method with continuity correction (= default of</span>
<span class="co">#                   prop.test())</span>
<span class="co"># &#39;binomial&#39;      : &quot;exact&quot; or Clopper-Pearson method interval (= binom.test())</span>
<span class="co"># &#39;binomial.midp&#39; : binomial mid-p &quot;quasi-exact&quot; interval</span>
<span class="co"># &#39;Jeffreys&#39;      : Jeffreys prior interval</span>
<span class="co"># &#39;Agresti-Coull&#39; : Agresti-Coull method adding z?/2 successes</span>
<span class="co"># &#39;Agresti.2_2    : Agresti-Coull method adding 2 successes and 2 failures</span>
<span class="co"># &#39;logit&#39;         : logit interval</span>
<span class="co">#</span>
<span class="co"># References:</span>
<span class="co"># Brown, L.D., Cai, T.T. &amp; DasGupta, A. (2001). Interval estimation for a</span>
<span class="co">#    binomial proportion. Statistical Science, 16, 101-133.</span>
<span class="co"># Newcombe, R.G. (1998). Two-sided confidence intervals for the single propor-</span>
<span class="co">#    tion: Comparison of seven methods. Statistics in Medicine, 17, 857-872.</span>

prop.CI =<span class="st"> </span>function(p,n,<span class="dt">alpha=</span>.<span class="dv">05</span>,<span class="dt">digits=</span><span class="dv">3</span>,<span class="dt">method=</span><span class="st">&quot;likelihood&quot;</span>)
{
<span class="co"># Asymptotic (or Wald) interval:</span>
  z =<span class="st"> </span><span class="kw">qnorm</span>(<span class="dv">1</span>-alpha/<span class="dv">2</span>)
  if (method==<span class="st">&#39;asymptotic&#39;</span>) {
    se =<span class="st"> </span><span class="kw">sqrt</span>(p*(<span class="dv">1</span>-p)/n)
    CI =<span class="st"> </span><span class="kw">list</span>(<span class="dt">p=</span>p,<span class="dt">CI=</span><span class="kw">c</span>((p-z*se),(p+z*se)),<span class="dt">n=</span>n,<span class="dt">level=</span><span class="dv">1</span>-alpha,<span class="dt">method=</span>method)
  }
<span class="co"># Asymptotic (or Wald-test) CIs with continuity correction:</span>
  if (method==<span class="st">&#39;asymptotic.cc&#39;</span>) {
    se =<span class="st"> </span><span class="kw">sqrt</span>(p*(<span class="dv">1</span>-p)/n)
    CI =<span class="st"> </span><span class="kw">list</span>(<span class="dt">p=</span>p,<span class="dt">CI=</span><span class="kw">c</span>((p-z*se<span class="dv">-1</span>/(<span class="dv">2</span>*n)),(p+z*se<span class="dv">+1</span>/(<span class="dv">2</span>*n))),<span class="dt">n=</span>n,<span class="dt">level=</span><span class="dv">1</span>-alpha,
              <span class="dt">method=</span>method)
  }
<span class="co"># Score test (or Wilson) interval:</span>
  if (method==<span class="st">&#39;score&#39;</span>) {
    term1 =<span class="st"> </span><span class="dv">2</span>*n*p +<span class="st"> </span>z**<span class="dv">2</span>
    term2 =<span class="st"> </span>z*<span class="kw">sqrt</span>(z**<span class="dv">2</span> +<span class="st"> </span><span class="dv">4</span>*n*p*(<span class="dv">1</span>-p))
    term3 =<span class="st"> </span><span class="dv">2</span>*(n +<span class="st"> </span>z**<span class="dv">2</span>)
    CI =<span class="st"> </span><span class="kw">list</span>(<span class="dt">p=</span>p,<span class="dt">CI=</span><span class="kw">c</span>((term1-term2)/term3,(term1+term2)/term3),<span class="dt">n=</span>n,
              <span class="dt">level=</span><span class="dv">1</span>-alpha,<span class="dt">method=</span>method)
  }
<span class="co"># Score test (or Wilson) interval with continuity correction:</span>
  if (method==<span class="st">&#39;score.cc&#39;</span>) {
    term1 =<span class="st"> </span><span class="dv">2</span>*n*p +<span class="st"> </span>z**<span class="dv">2</span>
    if (p&gt;<span class="dv">0</span>) {
    term2L =<span class="st"> </span>z*<span class="kw">sqrt</span>(z**<span class="dv">2</span> -<span class="st"> </span><span class="dv">2</span> -<span class="st"> </span><span class="dv">1</span>/n +<span class="st"> </span><span class="dv">4</span>*p*(n*(<span class="dv">1</span>-p)+<span class="dv">1</span>))
    }
    if (p&lt;<span class="dv">1</span>) {
    term2U =<span class="st"> </span>z*<span class="kw">sqrt</span>(z**<span class="dv">2</span> +<span class="st"> </span><span class="dv">2</span> -<span class="st"> </span><span class="dv">1</span>/n +<span class="st"> </span><span class="dv">4</span>*p*(n*(<span class="dv">1</span>-p)-<span class="dv">1</span>))
    }
    term3 =<span class="st"> </span><span class="dv">2</span>*(n +<span class="st"> </span>z**<span class="dv">2</span>)
    if ((p&gt;<span class="dv">0</span>) &amp;<span class="st"> </span>(p&lt;<span class="dv">1</span>)) {
      CI =<span class="st"> </span><span class="kw">list</span>(<span class="dt">p=</span>p,<span class="dt">CI=</span><span class="kw">c</span>((term1<span class="dv">-1</span>-term2L)/term3,(term1<span class="dv">+1</span>+term2U)/term3),<span class="dt">n=</span>n,
                <span class="dt">level=</span><span class="dv">1</span>-alpha,<span class="dt">method=</span>method)
    }
    if (p==<span class="dv">0</span>) {
      CI =<span class="st"> </span><span class="kw">list</span>(<span class="dt">p=</span>p,<span class="dt">CI=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dt">CIU=</span>(term1<span class="dv">+1</span>+term2U)/term3),<span class="dt">n=</span>n,<span class="dt">level=</span><span class="dv">1</span>-alpha,
                <span class="dt">method=</span>method)
    }
    if (p==<span class="dv">1</span>) {
      CI =<span class="st"> </span><span class="kw">list</span>(<span class="dt">p=</span>p,<span class="dt">CI=</span><span class="kw">c</span>((term1<span class="dv">-1</span>-term2L)/term3,<span class="dv">1</span>),<span class="dt">n=</span>n,<span class="dt">level=</span><span class="dv">1</span>-alpha,
                <span class="dt">method=</span>method)
    }
  }
<span class="co"># Binomial (&#39;exact&#39; or Clopper-Pearson) interval:</span>
  if (method==<span class="st">&#39;binomial&#39;</span>) {
    conf.int=<span class="kw">binom.test</span>(<span class="kw">round</span>(p*n),n,<span class="dt">conf.level=</span><span class="dv">1</span>-alpha)$conf.int
    CI =<span class="st"> </span><span class="kw">list</span>(<span class="dt">p=</span>p,<span class="dt">CI=</span><span class="kw">c</span>(conf.int[<span class="dv">1</span>],conf.int[<span class="dv">2</span>]),<span class="dt">n=</span>n,<span class="dt">level=</span><span class="dv">1</span>-alpha,<span class="dt">method=</span>method)
  }
<span class="co"># Binomial mid-p quasi-exact interval:</span>
  if (method==<span class="st">&#39;binomial.midp&#39;</span>) {
    x =<span class="st"> </span><span class="kw">round</span>(p*n)
    uplim =<span class="st"> </span><span class="dv">1</span>
    lowlim =<span class="st"> </span><span class="dv">0</span>
    if (x ==<span class="st"> </span><span class="dv">0</span>) uplim =<span class="st"> </span><span class="dv">1</span>-alpha**(<span class="dv">1</span>/n)
    if (x ==<span class="st"> </span>n) lowlim =<span class="st"> </span>alpha**(<span class="dv">1</span>/n)
    if (x &gt;<span class="st"> </span><span class="dv">0</span> &amp;<span class="st"> </span>x &lt;<span class="st"> </span>n) {
      pp =<span class="st"> </span><span class="kw">seq</span>(<span class="fl">0.000001</span>,<span class="fl">0.999999</span>,<span class="dt">length=</span><span class="dv">100000</span>)
      a2 =<span class="st"> </span><span class="fl">0.5</span>*<span class="kw">pbinom</span>(x<span class="dv">-1</span>,n,pp) +<span class="st"> </span><span class="fl">0.5</span>*<span class="kw">pbinom</span>(x,n,pp)
      uplim =<span class="st"> </span>pp[<span class="kw">max</span>(<span class="kw">which</span>(a2&gt;(alpha/<span class="dv">2</span>)))]
      lowlim =<span class="st"> </span>pp[<span class="kw">min</span>(<span class="kw">which</span>(a2&lt;(<span class="dv">1</span>-alpha/<span class="dv">2</span>)))]
    }
    CI =<span class="st"> </span><span class="kw">list</span>(<span class="dt">p=</span>p,<span class="dt">CI=</span><span class="kw">c</span>(lowlim,uplim),<span class="dt">n=</span>n,<span class="dt">level=</span><span class="dv">1</span>-alpha,<span class="dt">method=</span>method)
  }
<span class="co"># Log-likelihood-ratio interval:</span>
  if (method==<span class="st">&#39;likelihood&#39;</span>) {
    x =<span class="st"> </span><span class="kw">round</span>(p*n)
    k =<span class="st"> </span>-<span class="kw">qchisq</span>(<span class="dv">1</span>-alpha,<span class="dv">1</span>)/<span class="dv">2</span>
    pp =<span class="st"> </span><span class="kw">seq</span>(<span class="fl">0.000001</span>,<span class="fl">0.999999</span>,<span class="dt">length=</span><span class="dv">100000</span>)
    lik =<span class="st"> </span><span class="kw">dbinom</span>(x,<span class="dt">size=</span>n,pp)
    logLR =<span class="st"> </span><span class="kw">log</span>(lik/<span class="kw">max</span>(lik))
    conf.int=<span class="kw">range</span>(pp[logLR &gt;<span class="st"> </span>k])
    CI =<span class="st"> </span><span class="kw">list</span>(<span class="dt">p=</span>p,<span class="dt">CI=</span><span class="kw">c</span>(conf.int[<span class="dv">1</span>],conf.int[<span class="dv">2</span>]),<span class="dt">n=</span>n,<span class="dt">level=</span><span class="dv">1</span>-alpha,<span class="dt">method=</span>method)
  }
<span class="co"># Jeffreys prior interval:</span>
  if (method==<span class="st">&#39;Jeffreys&#39;</span>) {
    x =<span class="st"> </span><span class="kw">round</span>(p*n)
    conf.int=<span class="kw">qbeta</span>(<span class="kw">c</span>(alpha/<span class="dv">2</span>,<span class="dv">1</span>-alpha/<span class="dv">2</span>),x<span class="fl">+.5</span>,n-x<span class="fl">+.5</span>)
    CI =<span class="st"> </span><span class="kw">list</span>(<span class="dt">p=</span>p,<span class="dt">CI=</span><span class="kw">c</span>(conf.int[<span class="dv">1</span>],conf.int[<span class="dv">2</span>]),<span class="dt">n=</span>n,<span class="dt">level=</span><span class="dv">1</span>-alpha,<span class="dt">method=</span>method)
  }
<span class="co"># Agresti-Coull (adding z?/2 successes) interval</span>
<span class="co"># (see: http://www.stat.ufl.edu/~aa/cda/R/one_sample/R1/index.html )</span>
  if (method==<span class="st">&#39;Agresti-Coull&#39;</span>) {
    x =<span class="st"> </span><span class="kw">round</span>(p*n)
    tr =<span class="st"> </span>z**<span class="dv">2</span>
    suc =<span class="st"> </span>tr/<span class="dv">2</span>
    pp =<span class="st"> </span>(x+suc)/(n+tr)
    se =<span class="st"> </span><span class="kw">sqrt</span>(pp*(<span class="dv">1</span>-pp)/(n+tr))
    CI =<span class="st"> </span><span class="kw">list</span>(<span class="dt">p=</span>p,<span class="dt">CI=</span><span class="kw">c</span>((pp-z*se),(pp+z*se)),<span class="dt">n=</span>n,<span class="dt">level=</span><span class="dv">1</span>-alpha,<span class="dt">method=</span>method)
    if (CI$CI[<span class="dv">1</span>] &lt;<span class="st"> </span><span class="dv">0</span>) CI$CI[<span class="dv">1</span>]=<span class="dv">0</span>
    if (CI$CI[<span class="dv">2</span>] &gt;<span class="st"> </span><span class="dv">1</span>) CI$CI[<span class="dv">2</span>]=<span class="dv">1</span>
  }
<span class="co"># Agresti-Coull (adding 2 successes and 2 failures) interval:</span>
<span class="co"># (see: http://www.stat.ufl.edu/~aa/cda/R/one_sample/R1/index.html )</span>
  if (method==<span class="st">&#39;Agresti.2_2&#39;</span>) {
    x =<span class="st"> </span><span class="kw">round</span>(p*n)
    pp =<span class="st"> </span>(x<span class="dv">+2</span>)/(n<span class="dv">+4</span>)
    se =<span class="st"> </span><span class="kw">sqrt</span>(pp*(<span class="dv">1</span>-pp)/(n<span class="dv">+4</span>))
    CI =<span class="st"> </span><span class="kw">list</span>(<span class="dt">p=</span>p,<span class="dt">CI=</span><span class="kw">c</span>((pp-z*se),(pp+z*se)),<span class="dt">n=</span>n,<span class="dt">level=</span><span class="dv">1</span>-alpha,<span class="dt">method=</span>method)
    if (CI$CI[<span class="dv">1</span>] &lt;<span class="st"> </span><span class="dv">0</span>) CI$CI[<span class="dv">1</span>]=<span class="dv">0</span>
    if (CI$CI[<span class="dv">2</span>] &gt;<span class="st"> </span><span class="dv">1</span>) CI$CI[<span class="dv">2</span>]=<span class="dv">1</span>
  }
<span class="co"># Logit interval:</span>
  if (method==<span class="st">&#39;logit&#39;</span>) {
    lambda =<span class="st"> </span><span class="kw">log</span>(p/(<span class="dv">1</span>-p))
    x =<span class="st"> </span><span class="kw">round</span>(p*n)
    V =<span class="st"> </span>n/(x*(n-x))
    conf.int =<span class="st"> </span>(<span class="kw">c</span>(lambda -<span class="st"> </span>z*<span class="kw">sqrt</span>(V),lambda +<span class="st"> </span>z*<span class="kw">sqrt</span>(V)))
    conf.int =<span class="st"> </span><span class="kw">exp</span>(conf.int)/(<span class="dv">1</span>+<span class="kw">exp</span>(conf.int))
    CI =<span class="st"> </span><span class="kw">list</span>(<span class="dt">p=</span>p,<span class="dt">CI=</span><span class="kw">c</span>(conf.int[<span class="dv">1</span>],conf.int[<span class="dv">2</span>]),<span class="dt">n=</span>n,<span class="dt">level=</span><span class="dv">1</span>-alpha,<span class="dt">method=</span>method)
  }
  <span class="kw">cat</span>(<span class="st">&#39;p ? &#39;</span>,<span class="dv">100</span>*(<span class="dv">1</span>-alpha),<span class="st">&#39;%-CI = &#39;</span>,<span class="kw">round</span>(p,digits),<span class="st">&#39; (&#39;</span>,
      <span class="kw">round</span>(CI$CI[<span class="dv">1</span>],digits),<span class="st">&#39;; &#39;</span>,<span class="kw">round</span>(CI$CI[<span class="dv">2</span>],digits),<span class="st">&#39;)</span><span class="ch">\n</span><span class="st">&#39;</span>,<span class="dt">sep=</span><span class="st">&#39;&#39;</span>)
  CI
}

<span class="co"># The following example  reproduces the data of Table I of Newcombe (1998,</span>
<span class="co"># p. 861). Other methods to calculate confidence intervals of a single propor-</span>
<span class="co"># tion as discussed in Brown et al. (2001) will also be demonstrated.</span>
<span class="co">#</span>
<span class="co"># To run, de-comment the following line:</span>
<span class="co"># source(&quot;http://www2.jura.uni-hamburg.de/instkrim/kriminologie/Mitarbeiter/Enzmann/Software/ex_prop.CI.r&quot;)</span></code></pre></div>
<p>I will use this code to calculate the confidence intervals for the binomial proportion if I get 2 successes out of 100 trials.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ci.1C1 =<span class="kw">prop.CI</span>(<span class="dv">2</span>/<span class="dv">100</span>,<span class="dv">100</span>,<span class="dt">digits=</span><span class="dv">4</span>,<span class="dt">method=</span><span class="st">&#39;asymptotic&#39;</span>)$CI</code></pre></div>
<pre><code>## p ? 95%-CI = 0.02 (-0.0074; 0.0474)</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ci.2C1 =<span class="kw">prop.CI</span>(<span class="dv">2</span>/<span class="dv">100</span>,<span class="dv">100</span>,<span class="dt">digits=</span><span class="dv">4</span>,<span class="dt">method=</span><span class="st">&#39;asymptotic.cc&#39;</span>)$CI</code></pre></div>
<pre><code>## p ? 95%-CI = 0.02 (-0.0124; 0.0524)</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ci.3C1 =<span class="kw">prop.CI</span>(<span class="dv">2</span>/<span class="dv">100</span>,<span class="dv">100</span>,<span class="dt">digits=</span><span class="dv">4</span>,<span class="dt">method=</span><span class="st">&#39;score&#39;</span>)$CI</code></pre></div>
<pre><code>## p ? 95%-CI = 0.02 (0.0055; 0.07)</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ci.4C1 =<span class="kw">prop.CI</span>(<span class="dv">2</span>/<span class="dv">100</span>,<span class="dv">100</span>,<span class="dt">digits=</span><span class="dv">4</span>,<span class="dt">method=</span><span class="st">&#39;score.cc&#39;</span>)$CI</code></pre></div>
<pre><code>## p ? 95%-CI = 0.02 (0.0035; 0.0774)</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ci.5C1 =<span class="kw">prop.CI</span>(<span class="dv">2</span>/<span class="dv">100</span>,<span class="dv">100</span>,<span class="dt">digits=</span><span class="dv">4</span>,<span class="dt">method=</span><span class="st">&#39;binomial&#39;</span>)$CI</code></pre></div>
<pre><code>## p ? 95%-CI = 0.02 (0.0024; 0.0704)</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ci.6C1 =<span class="kw">prop.CI</span>(<span class="dv">2</span>/<span class="dv">100</span>,<span class="dv">100</span>,<span class="dt">digits=</span><span class="dv">4</span>,<span class="dt">method=</span><span class="st">&#39;binomial.midp&#39;</span>)$CI</code></pre></div>
<pre><code>## p ? 95%-CI = 0.02 (0.0034; 0.0645)</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ci.7C1 =<span class="kw">prop.CI</span>(<span class="dv">2</span>/<span class="dv">100</span>,<span class="dv">100</span>,<span class="dt">digits=</span><span class="dv">4</span>)$CI</code></pre></div>
<pre><code>## p ? 95%-CI = 0.02 (0.0034; 0.0605)</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ci.8C1 =<span class="kw">prop.CI</span>(<span class="dv">2</span>/<span class="dv">100</span>,<span class="dv">100</span>,<span class="dt">digits=</span><span class="dv">4</span>,<span class="dt">method=</span><span class="st">&#39;Jeffreys&#39;</span>)$CI</code></pre></div>
<pre><code>## p ? 95%-CI = 0.02 (0.0042; 0.0626)</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ci.9C1 =<span class="kw">prop.CI</span>(<span class="dv">2</span>/<span class="dv">100</span>,<span class="dv">100</span>,<span class="dt">digits=</span><span class="dv">4</span>,<span class="dt">method=</span><span class="st">&#39;Agresti-Coull&#39;</span>)$CI</code></pre></div>
<pre><code>## p ? 95%-CI = 0.02 (0.0011; 0.0744)</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ci.10C1=<span class="kw">prop.CI</span>(<span class="dv">2</span>/<span class="dv">100</span>,<span class="dv">100</span>,<span class="dt">digits=</span><span class="dv">4</span>,<span class="dt">method=</span><span class="st">&#39;Agresti.2_2&#39;</span>)$CI</code></pre></div>
<pre><code>## p ? 95%-CI = 0.02 (0.0015; 0.0754)</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ci.11C1=<span class="kw">prop.CI</span>(<span class="dv">2</span>/<span class="dv">100</span>,<span class="dv">100</span>,<span class="dt">digits=</span><span class="dv">4</span>,<span class="dt">method=</span><span class="st">&#39;logit&#39;</span>)$CI</code></pre></div>
<pre><code>## p ? 95%-CI = 0.02 (0.005; 0.0764)</code></pre>
<p>We see that most of these methods correctly constrain the CIs to be bounded by [0,1].</p>
</div>
<div id="comparing-two-distributions" class="section level2">
<h2><span class="header-section-number">10.3</span> Comparing two distributions</h2>
<p>To perform the K-S test to compare two distributions, we need to simulate some data. We will plot their (empirical) cumulative distributions using the function &quot;ecdf&quot;.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x&lt;-<span class="kw">rnorm</span>(<span class="dv">50</span>,<span class="dt">mean=</span><span class="dv">0</span>,<span class="dt">sd=</span><span class="dv">1</span>)
y&lt;-<span class="kw">runif</span>(<span class="dv">30</span>,<span class="dv">0</span>,<span class="dv">1</span>)
<span class="kw">ks.test</span>(x,y)</code></pre></div>
<pre><code>## 
##  Two-sample Kolmogorov-Smirnov test
## 
## data:  x and y
## D = 0.58, p-value = 2.381e-06
## alternative hypothesis: two-sided</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(<span class="kw">ecdf</span>(x))
<span class="kw">lines</span>(<span class="kw">ecdf</span>(y),<span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</code></pre></div>
<p><img src="Week-5-lab_files/figure-html/unnamed-chunk-27-1.png" width="672" /></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="week-5-lecture.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="week-6-lecture.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
